{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.autoresmaker.main import AutoResMaker\n",
    "from src.autoresevaluator.main import AutoResEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "arm = AutoResMaker(\n",
    "    #llm_name = 'google',\n",
    "    llm_name = 'openai',\n",
    "    base_model='cnn',\n",
    "    dataset_name='cifar10',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "elemental_method1 = \"\"\"\n",
    "<ELEMENTAL_METHOD name=\"Momentum Calculation and Application\">\n",
    "This method involves computing the moving average of the gradients (also known as the first moment) and using this to update the model parameters. It's a technique used to accelerate gradients vectors in the right directions, thus leading to faster converging. It is widely used in optimization algorithms, including as a component of the Adam optimizer.\n",
    "\n",
    "<PYTHON>\n",
    "def momentum_update(lr, beta1, params, objective, weight_decay, epsilon):\n",
    "    m = 0  # first moment\n",
    "    theta = params\n",
    "    while True:\n",
    "        g = np.gradient(objective(theta), theta)  # gradient of objective function\n",
    "        if weight_decay != 0:\n",
    "            g += weight_decay * theta\n",
    "        m = beta1 * m + (1 - beta1) * g\n",
    "        theta -= lr * m\n",
    "        yield theta\n",
    "</PYTHON>\n",
    "</ELEMENTAL_METHOD>\n",
    "\"\"\"\n",
    "\n",
    "elemental_method2 = \"\"\"\n",
    "<ELEMENTAL_METHOD name=\"RMSprop Calculation and Application\">\n",
    "This method involves computing the moving average of the squared gradients (also known as the second moment), applying bias correction, and using this to adjust the parameter updates. It's designed to adapt the learning rate for each parameter, helping to mitigate the vanishing or exploding gradient problem. This technique is a core part of the Adam optimizer.\n",
    "\n",
    "<PYTHON>\n",
    "def rmsprop_update(lr, beta2, params, objective, weight_decay, epsilon):\n",
    "    v = 0  # second moment\n",
    "    theta = params\n",
    "    while True:\n",
    "        g = np.gradient(objective(theta), theta)  # gradient of objective function\n",
    "        if weight_decay != 0:\n",
    "            g += weight_decay * theta\n",
    "        v = beta2 * v + (1 - beta2) * g**2\n",
    "        v_hat = v / (1 - beta2)\n",
    "        theta -= lr * g / (np.sqrt(v_hat) + epsilon)\n",
    "        yield theta\n",
    "</PYTHON>\n",
    "</ELEMENTAL_METHOD>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = arm.exec(elemental_method1, elemental_method2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "\n",
      "class Net(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Net, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = x.view(-1, 16 * 5 * 5)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        return x\n",
      "\n",
      "def train_with_adam(trainloader, params, device):\n",
      "    net = Net()\n",
      "    net.to(device)  # Move the model to the specified device\n",
      "\n",
      "    criterion = nn.CrossEntropyLoss()\n",
      "\n",
      "    # Adam parameters\n",
      "    beta1 = 0.9\n",
      "    beta2 = 0.999\n",
      "    epsilon = 1e-8\n",
      "    lr = params['lr']\n",
      "    weight_decay = params.get('weight_decay', 0)\n",
      "\n",
      "    # Initialize Adam parameters for each parameter in the model\n",
      "    m = {name: torch.zeros_like(param.data) for name, param in net.named_parameters()}\n",
      "    v = {name: torch.zeros_like(param.data) for name, param in net.named_parameters()}\n",
      "    t = 0\n",
      "\n",
      "    for epoch in range(1):  # loop over the dataset multiple times\n",
      "        for i, data in enumerate(trainloader, 0):\n",
      "            inputs, labels = data[0].to(device), data[1].to(device)  # Move inputs and labels to the specified device\n",
      "\n",
      "            net.zero_grad()  # zero the parameter gradients\n",
      "\n",
      "            outputs = net(inputs)  # forward\n",
      "            loss = criterion(outputs, labels)\n",
      "            loss.backward()  # backward\n",
      "\n",
      "            t += 1\n",
      "            with torch.no_grad():\n",
      "                for name, param in net.named_parameters():\n",
      "                    if param.grad is not None:\n",
      "                        g = param.grad.data\n",
      "                        if weight_decay != 0:\n",
      "                            g += weight_decay * param.data\n",
      "                        m[name] = beta1 * m[name] + (1 - beta1) * g\n",
      "                        v[name] = beta2 * v[name] + (1 - beta2) * g**2\n",
      "                        m_hat = m[name] / (1 - beta1**t)\n",
      "                        v_hat = v[name] / (1 - beta2**t)\n",
      "                        param.data -= lr * m_hat / (torch.sqrt(v_hat) + epsilon)\n",
      "\n",
      "def model(trainloader, testloader, params):\n",
      "    if torch.cuda.is_available():\n",
      "        print(\"CUDA (GPU support) is available in this environment.\")\n",
      "        device = torch.device(\"cuda\")\n",
      "    else:\n",
      "        device = torch.device(\"cpu\")\n",
      "    net = train_with_adam(trainloader, params, device)\n",
      "    y_pred = test(net, testloader, device)\n",
      "    return y_pred\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'learning_rate': {'type': 'log_float', 'args': [1e-8, 0.1]},\n",
    "    'iterations': {'type': 'log_float', 'args': [100, 1000]},\n",
    "    \"lambda\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "are = AutoResEvaluator(\n",
    "    task_type='binary classification',\n",
    "    dataset_name='titanic',\n",
    "    model_path='/Users/tanakatouma/vscode/autores-evaluator/test/logistic_regression.py',\n",
    "    #model_path='/Users/tanakatouma/vscode/autores-evaluator/test/improve_logistic_regression.py',\n",
    "    params=params,\n",
    "    valuation_index='roc_auc'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "are.exec()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
